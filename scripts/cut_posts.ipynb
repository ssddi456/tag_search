{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PIL\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "\n",
    "all_posts = '../posts/posts.json'\n",
    "list_file_name = '../posts/files.json'\n",
    "\n",
    "prefix = '{\"id\":'\n",
    "prefix_len = len(prefix)\n",
    "tag_string = 'tag_string\":\"'\n",
    "tag_string_len = len(tag_string)\n",
    "\n",
    "def get_line_info(line):\n",
    "    first_comma = line.find(',', prefix_len)\n",
    "    id = line[prefix_len:first_comma]\n",
    "    tag_string_start = line.find(tag_string, first_comma)\n",
    "    tag_string_end = line.find('\"', tag_string_start + tag_string_len)\n",
    "\n",
    "    tag_strings = line[tag_string_start + tag_string_len:tag_string_end]\n",
    "    return id, tag_strings\n",
    "\n",
    "def get_line_id(line):\n",
    "    first_comma = line.find(',', prefix_len)\n",
    "    id = line[prefix_len:first_comma]\n",
    "    return id\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(image_url):\n",
    "    data = requests.get(image_url).content\n",
    "    img = Image.open(BytesIO(data))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "def display_multiple_images(image_urls, offset=0):\n",
    "    if len(image_urls) == 0:\n",
    "        print('No images to display')\n",
    "        return\n",
    "    if len(image_urls) > 12:\n",
    "        # cut the list to 12 by batch\n",
    "        for i in range(0, len(image_urls), 12):\n",
    "            display_multiple_images(image_urls[i:i + 12], offset=i)\n",
    "        return\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    sqrt = int(len(image_urls) ** 0.5)\n",
    "\n",
    "    columns = min(5, sqrt)\n",
    "    rows = len(image_urls) // columns + 1\n",
    "    for i in range(1, columns * rows + 1):\n",
    "        if i > len(image_urls):\n",
    "            break\n",
    "        data = requests.get(image_urls[i - 1]).content\n",
    "        img = Image.open(BytesIO(data))\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(i + offset)\n",
    "    plt.show()\n",
    "\n",
    "def match_tag(tag: str, tag_strings: str):\n",
    "    padding = ' ' + tag + ' '\n",
    "    padded_tag_strings = ' ' + tag_strings + ' '\n",
    "    return padded_tag_strings.find(padding) != -1\n",
    "\n",
    "def match_tags(tags: list, tag_strings: str):\n",
    "    for tag in tags:\n",
    "        if tag not in tag_strings:\n",
    "            return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(match_tag('a', 'a b c') == True)\n",
    "print(match_tag('b', 'a b c') == True)\n",
    "print(match_tag('c', 'a b c') == True)\n",
    "print(match_tag('e', 'a be c') == False)\n",
    "print(match_tag('e', 'a be c e') == True)\n",
    "print(match_tag('d', 'a b c') == False)\n",
    "\n",
    "print(match_tags(['a', 'b'], '1 a b c ') == True)\n",
    "print(match_tags(['a', 'b'], '1 a b c d ') == True)\n",
    "print(match_tags(['a', 'b'], '1 a b ') == True)\n",
    "print(match_tags(['a', 'e'], '1 a b ') == False)\n",
    "print(match_tags(['a', 'e'], '1 a b c d e ') == True)\n",
    "print(match_tags(['a', 'e'], '1 a be c d e f ') == True)\n",
    "print(match_tags(['a', 'ef'], '1 a be c d ef ') == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "line_count = 0\n",
    "max_count = 10\n",
    "id_to_found = '7344500'\n",
    "with open(all_posts, 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        pass\n",
    "    # last_line\n",
    "    first_comma = line.find(',', prefix_len)\n",
    "    id = line[prefix_len:first_comma]\n",
    "    print(id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(all_posts, 'r', encoding = 'utf-8') as f:\n",
    "    line = f.readline()\n",
    "    print(line)\n",
    "    print(get_line_info(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the file into files each containing 200000 posts, and only keep id and tag_string property\n",
    "# write the files to disk, add file info to a file list, and write the file list to disk\n",
    "\n",
    "\n",
    "current_file_piece = 0\n",
    "current_file_name = f'../posts/posts_{current_file_piece}.json'\n",
    "current_tag_file_name = f'../posts/posts_{current_file_piece}_tags.json'\n",
    "\n",
    "current_file = open(current_file_name, 'w', encoding = 'utf-8')\n",
    "current_tag_file = open(current_tag_file_name, 'w', encoding = 'utf-8')\n",
    "\n",
    "list_file = open(list_file_name, 'w', encoding = 'utf-8')\n",
    "\n",
    "line_count = 0\n",
    "start_id = None\n",
    "end_id = None\n",
    "line_count_max = 200000\n",
    "with open(all_posts, 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        line_count += 1\n",
    "        if line_count > line_count_max:\n",
    "            current_file.close()\n",
    "            current_tag_file.close()\n",
    "\n",
    "            list_file.write(json.dumps({\n",
    "                'file': current_file_name,\n",
    "                'tag_file': current_tag_file_name,\n",
    "                'start_id': start_id,\n",
    "                'end_id': end_id,\n",
    "            }) + '\\n')\n",
    "\n",
    "            current_file_piece += 1\n",
    "            current_file_name = f'../posts/posts_{current_file_piece}.json'\n",
    "            current_tag_file_name = f'../posts/posts_{current_file_piece}_tags.json'\n",
    "\n",
    "\n",
    "            start_id = None\n",
    "            current_file = open(current_file_name, 'w', encoding = 'utf-8')\n",
    "            current_tag_file = open(current_tag_file_name, 'w', encoding = 'utf-8')\n",
    "            line_count = 0\n",
    "\n",
    "        id, tag_strings = get_line_info(line)\n",
    "        if start_id is None:\n",
    "            start_id = id\n",
    "        end_id = id\n",
    "\n",
    "        current_file_size = current_file.tell()\n",
    "        line_binary_len = current_file.write(line)\n",
    "        current_tag_file.write(f'{{\"id\":{id},\"tag_string\":\" {tag_strings} \", \"offset\":[{current_file_size},{line_binary_len}]}}\\n')\n",
    "\n",
    "current_tag_file.close()\n",
    "current_file.close()\n",
    "list_file.write(json.dumps({\n",
    "    'file': current_file_name,\n",
    "    'tag_file': current_tag_file_name,\n",
    "    'start_id': start_id,\n",
    "    'end_id': end_id,\n",
    "}) + '\\n')\n",
    "\n",
    "list_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "def search_file_with_tag(tag: list, files_queue, res_queue, thread_id):\n",
    "    print('start', thread_id)\n",
    "    while not files_queue.empty():\n",
    "        file_info = json.loads(files_queue.get())\n",
    "        # print('searching', file_info['file'])\n",
    "        current_piece_info = []\n",
    "        with open('.' + file_info['tag_file'], 'r', encoding = 'utf-8') as tag_file:\n",
    "            for tag_line in tag_file:\n",
    "                if match_tags(tag, tag_line):\n",
    "                    line_info = json.loads(tag_line)\n",
    "                    # print('found', line_info['id'])\n",
    "                    current_piece_info.append(line_info)\n",
    "                    pass\n",
    "\n",
    "        if len(current_piece_info) == 0:\n",
    "            files_queue.task_done()\n",
    "            # print('done', file_info['file'])\n",
    "            continue\n",
    "\n",
    "        # print('found', len(current_piece_info), 'lines')\n",
    "\n",
    "        with open('.' + file_info['file'], 'r', encoding = 'utf-8') as post_file:\n",
    "            for line_info in current_piece_info:\n",
    "                # print('seeking', file_info['file'], line_info)\n",
    "                post_file.seek(line_info['offset'][0])\n",
    "                line = post_file.read(line_info['offset'][1])\n",
    "                # print('found line', line)\n",
    "                res_queue.put(line)\n",
    "\n",
    "        files_queue.task_done()\n",
    "        # print('done', file_info['file'])\n",
    "    print('done all', thread_id)\n",
    "\n",
    "def map_reduce_search_tag(tag: list, max_count: int = 10, start_id: int = None, end_id: int = None):\n",
    "\n",
    "    files_queue = Queue()\n",
    "    res_queue = Queue()\n",
    "    threads = []\n",
    "\n",
    "    file_added_count = 0\n",
    "    with open(list_file_name, 'r', encoding = 'utf-8') as f:\n",
    "        # search parallel\n",
    "\n",
    "        for line in f:\n",
    "            file_info = json.loads(line)\n",
    "            if start_id is not None and int(file_info['end_id']) < start_id:\n",
    "                continue\n",
    "            if end_id is not None and int(file_info['start_id']) > end_id:\n",
    "                continue\n",
    "\n",
    "            files_queue.put(line)\n",
    "            file_added_count += 1\n",
    "\n",
    "    print('file added', file_added_count)\n",
    "    updated_tags = [' ' + x + ' ' for x in tag]\n",
    "    for i in range(8):\n",
    "        t = threading.Thread(target=search_file_with_tag, args=(updated_tags, files_queue, res_queue, i))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    print('threads started')\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "        print('threads done')\n",
    "\n",
    "    ret = []\n",
    "\n",
    "    while not res_queue.empty():\n",
    "        data = res_queue.get()\n",
    "        json_data = json.loads(data)\n",
    "        if start_id is not None and int(json_data['id']) < start_id:\n",
    "            continue\n",
    "        if end_id is not None and int(json_data['id']) > end_id:\n",
    "            continue\n",
    "        ret.append(json_data)\n",
    "\n",
    "    return ret\n",
    "\n",
    "query = [\n",
    "    'simple_background',\n",
    "]\n",
    "search_res = map_reduce_search_tag(query, max_count=40,)\n",
    "img_idx = 0\n",
    "for file_info in search_res:\n",
    "    print(file_info['id'])\n",
    "    print(file_info['tag_string'])\n",
    "    if 'large_file_url' in file_info:\n",
    "        print(img_idx, file_info['large_file_url'])\n",
    "        img_idx += 1\n",
    "    else:\n",
    "        print('no large file url', json.dumps(file_info))\n",
    "    # preview the image\n",
    "\n",
    "display_multiple_images([file_info['large_file_url'] for file_info in search_res if 'large_file_url' in file_info])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
